{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "json_folder = \"../data/preprocessed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = \"../data/preprocessed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = [f for f in os.listdir(json_folder) if f.endswith(\".json\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id         sender                 timestamp  \\\n",
      "0  1387 -1001838667733 2025-01-20 09:39:41+00:00   \n",
      "1  1386 -1001838667733 2025-01-20 07:53:19+00:00   \n",
      "2  1385 -1001838667733 2025-01-20 05:07:16+00:00   \n",
      "3  1383 -1001838667733 2025-01-18 17:49:31+00:00   \n",
      "4  1382 -1001838667733 2025-01-18 10:45:20+00:00   \n",
      "\n",
      "                                                text  media  \\\n",
      "0  áŠ¥áŠá‹šáˆ…áŠ• áŒ«áˆ›á‹á‰½ facebook áŠ¥áŠ“ instagram áˆ‹á‹­ áˆ›áˆµá‰³á‹ˆá‰‚á‹« áŠ á‹­á‰³...    NaN   \n",
      "1                                     á‹›áˆ¬ áˆ™áˆ‰á‰€áŠ• áŠ­áá‰µ áŠáŠ•    NaN   \n",
      "2  áŠ¥áŠá‹šáˆ…áŠ• áŒ«áˆ›á‹á‰½ facebook áŠ¥áŠ“ instagram áˆ‹á‹­ áˆ›áˆµá‰³á‹ˆá‰‚á‹« áŠ á‹­á‰³...    NaN   \n",
      "3  ğŸ‘Ÿ brand :- PRERA COMFI WEAR\\nğŸ“ sizeÂ  :-Â  39,40...    NaN   \n",
      "4  ğŸ‘Ÿ brand :- CLASSIC DOSIR\\nğŸ“ sizeÂ  :-Â  39,40,41...    NaN   \n",
      "\n",
      "                                        cleaned_text      channel  \n",
      "0  áŠ¥áŠá‹šáˆ…áŠ• áŒ«áˆ›á‹á‰½ facebook áŠ¥áŠ“ instagram áˆ‹á‹­ áˆ›áˆµá‰³á‹ˆá‰‚á‹« áŠ á‹­á‰³...  belaclassic  \n",
      "1                                     á‹›áˆ¬ áˆ™áˆ‰á‰€áŠ• áŠ­áá‰µ áŠáŠ•  belaclassic  \n",
      "2  áŠ¥áŠá‹šáˆ…áŠ• áŒ«áˆ›á‹á‰½ facebook áŠ¥áŠ“ instagram áˆ‹á‹­ áˆ›áˆµá‰³á‹ˆá‰‚á‹« áŠ á‹­á‰³...  belaclassic  \n",
      "3  brand PRERA COMFI WEAR size 394041424344 á‹‹áŒ‹ 38...  belaclassic  \n",
      "4  brand CLASSIC DOSIR size 394041424344 á‹‹áŒ‹ 3900 ...  belaclassic  \n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop through each JSON file, read it, and append it to the list\n",
    "for file in json_files:\n",
    "    file_path = os.path.join(json_folder, file)\n",
    "    # Read the JSON file into a DataFrame\n",
    "    df = pd.read_json(file_path, orient='records', encoding='utf-8')\n",
    "    # Add a column to identify the source channel\n",
    "    df['channel'] = os.path.splitext(file)[0]  # File name without extension\n",
    "    # Append the DataFrame to the list\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Merge all DataFrames into a single DataFrame\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = merged_df[[\"cleaned_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>áŠ¥áŠá‹šáˆ…áŠ• áŒ«áˆ›á‹á‰½ facebook áŠ¥áŠ“ instagram áˆ‹á‹­ áˆ›áˆµá‰³á‹ˆá‰‚á‹« áŠ á‹­á‰³...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>á‹›áˆ¬ áˆ™áˆ‰á‰€áŠ• áŠ­áá‰µ áŠáŠ•</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>áŠ¥áŠá‹šáˆ…áŠ• áŒ«áˆ›á‹á‰½ facebook áŠ¥áŠ“ instagram áˆ‹á‹­ áˆ›áˆµá‰³á‹ˆá‰‚á‹« áŠ á‹­á‰³...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>brand PRERA COMFI WEAR size 394041424344 á‹‹áŒ‹ 38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brand CLASSIC DOSIR size 394041424344 á‹‹áŒ‹ 3900 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>brand CIZAR size 394041424344 á‹‹áŒ‹ 3600 áˆµáˆáŠ­ 0944...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>brand FABANNU GENUINE size 394041424344 á‹‹áŒ‹ 380...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>á‹­áˆ…áŠ• áŒ«áˆ› á‰µáŠ•áˆ½ á‰áŒ¥áˆ­ áˆáˆáŒ‹á‰½áˆ á‰ á‰°á‹°áŒ‹áŒ‹áˆš áˆ˜áŒ¥á‰³á‰½áˆ áˆ‹áŒ£á‰¹á‰µ á‰ áˆ™áˆ‰ áˆ¶áˆµá‰µ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>brand AIRFORCE 1 LOW size 394041424344 á‹‹áŒ‹ 3900...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>brand PERSIAN MOCCASIN size 394041424344 á‹‹áŒ‹ 39...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text\n",
       "0  áŠ¥áŠá‹šáˆ…áŠ• áŒ«áˆ›á‹á‰½ facebook áŠ¥áŠ“ instagram áˆ‹á‹­ áˆ›áˆµá‰³á‹ˆá‰‚á‹« áŠ á‹­á‰³...\n",
       "1                                     á‹›áˆ¬ áˆ™áˆ‰á‰€áŠ• áŠ­áá‰µ áŠáŠ•\n",
       "2  áŠ¥áŠá‹šáˆ…áŠ• áŒ«áˆ›á‹á‰½ facebook áŠ¥áŠ“ instagram áˆ‹á‹­ áˆ›áˆµá‰³á‹ˆá‰‚á‹« áŠ á‹­á‰³...\n",
       "3  brand PRERA COMFI WEAR size 394041424344 á‹‹áŒ‹ 38...\n",
       "4  brand CLASSIC DOSIR size 394041424344 á‹‹áŒ‹ 3900 ...\n",
       "5  brand CIZAR size 394041424344 á‹‹áŒ‹ 3600 áˆµáˆáŠ­ 0944...\n",
       "6  brand FABANNU GENUINE size 394041424344 á‹‹áŒ‹ 380...\n",
       "7  á‹­áˆ…áŠ• áŒ«áˆ› á‰µáŠ•áˆ½ á‰áŒ¥áˆ­ áˆáˆáŒ‹á‰½áˆ á‰ á‰°á‹°áŒ‹áŒ‹áˆš áˆ˜áŒ¥á‰³á‰½áˆ áˆ‹áŒ£á‰¹á‰µ á‰ áˆ™áˆ‰ áˆ¶áˆµá‰µ...\n",
       "8  brand AIRFORCE 1 LOW size 394041424344 á‹‹áŒ‹ 3900...\n",
       "9  brand PERSIAN MOCCASIN size 394041424344 á‹‹áŒ‹ 39..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.dropna(subset=\"cleaned_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>áŠ¥áŠá‹šáˆ…áŠ• áŒ«áˆ›á‹á‰½ facebook áŠ¥áŠ“ instagram áˆ‹á‹­ áˆ›áˆµá‰³á‹ˆá‰‚á‹« áŠ á‹­á‰³...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>á‹›áˆ¬ áˆ™áˆ‰á‰€áŠ• áŠ­áá‰µ áŠáŠ•</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>áŠ¥áŠá‹šáˆ…áŠ• áŒ«áˆ›á‹á‰½ facebook áŠ¥áŠ“ instagram áˆ‹á‹­ áˆ›áˆµá‰³á‹ˆá‰‚á‹« áŠ á‹­á‰³...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>brand PRERA COMFI WEAR size 394041424344 á‹‹áŒ‹ 38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brand CLASSIC DOSIR size 394041424344 á‹‹áŒ‹ 3900 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>brand CIZAR size 394041424344 á‹‹áŒ‹ 3600 áˆµáˆáŠ­ 0944...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>brand FABANNU GENUINE size 394041424344 á‹‹áŒ‹ 380...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>á‹­áˆ…áŠ• áŒ«áˆ› á‰µáŠ•áˆ½ á‰áŒ¥áˆ­ áˆáˆáŒ‹á‰½áˆ á‰ á‰°á‹°áŒ‹áŒ‹áˆš áˆ˜áŒ¥á‰³á‰½áˆ áˆ‹áŒ£á‰¹á‰µ á‰ áˆ™áˆ‰ áˆ¶áˆµá‰µ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>brand AIRFORCE 1 LOW size 394041424344 á‹‹áŒ‹ 3900...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>brand PERSIAN MOCCASIN size 394041424344 á‹‹áŒ‹ 39...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text\n",
       "0  áŠ¥áŠá‹šáˆ…áŠ• áŒ«áˆ›á‹á‰½ facebook áŠ¥áŠ“ instagram áˆ‹á‹­ áˆ›áˆµá‰³á‹ˆá‰‚á‹« áŠ á‹­á‰³...\n",
       "1                                     á‹›áˆ¬ áˆ™áˆ‰á‰€áŠ• áŠ­áá‰µ áŠáŠ•\n",
       "2  áŠ¥áŠá‹šáˆ…áŠ• áŒ«áˆ›á‹á‰½ facebook áŠ¥áŠ“ instagram áˆ‹á‹­ áˆ›áˆµá‰³á‹ˆá‰‚á‹« áŠ á‹­á‰³...\n",
       "3  brand PRERA COMFI WEAR size 394041424344 á‹‹áŒ‹ 38...\n",
       "4  brand CLASSIC DOSIR size 394041424344 á‹‹áŒ‹ 3900 ...\n",
       "5  brand CIZAR size 394041424344 á‹‹áŒ‹ 3600 áˆµáˆáŠ­ 0944...\n",
       "6  brand FABANNU GENUINE size 394041424344 á‹‹áŒ‹ 380...\n",
       "7  á‹­áˆ…áŠ• áŒ«áˆ› á‰µáŠ•áˆ½ á‰áŒ¥áˆ­ áˆáˆáŒ‹á‰½áˆ á‰ á‰°á‹°áŒ‹áŒ‹áˆš áˆ˜áŒ¥á‰³á‰½áˆ áˆ‹áŒ£á‰¹á‰µ á‰ áˆ™áˆ‰ áˆ¶áˆµá‰µ...\n",
       "8  brand AIRFORCE 1 LOW size 394041424344 á‹‹áŒ‹ 3900...\n",
       "9  brand PERSIAN MOCCASIN size 394041424344 á‹‹áŒ‹ 39..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_english(text):\n",
    "    # This regex removes English alphabets, but keeps numbers and specific punctuation\n",
    "    return re.sub(r'[A-Za-zá¦á¤á¥á¦á§á¨á©áªá«á¬á­á®á¯á°á±á²á³á´áµá¶á·á¸á¹áºá»\\._]', '', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_df[\"cleaned_text\"] = filtered_df[\"cleaned_text\"].apply(remove_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>áŠ¥áŠá‹šáˆ…áŠ• áŒ«áˆ›á‹á‰½  áŠ¥áŠ“  áˆ‹á‹­ áˆ›áˆµá‰³á‹ˆá‰‚á‹« áŠ á‹­á‰³á‰½áˆ  áˆµá‰µáŒˆá‰¡ á‹«áŒ£á‰½áˆá‰µ á‰ áˆ™...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>á‹›áˆ¬ áˆ™áˆ‰á‰€áŠ• áŠ­áá‰µ áŠáŠ•</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>áŠ¥áŠá‹šáˆ…áŠ• áŒ«áˆ›á‹á‰½  áŠ¥áŠ“  áˆ‹á‹­ áˆ›áˆµá‰³á‹ˆá‰‚á‹« áŠ á‹­á‰³á‰½áˆ  áˆµá‰µáŒˆá‰¡ á‹«áŒ£á‰½áˆá‰µ á‰ áˆ™...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>394041424344 á‹‹áŒ‹ 3800 áˆµáˆáŠ­ 0944222069 á‰ á‹áˆµáŒ¥ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>394041424344 á‹‹áŒ‹ 3900 áˆµáˆáŠ­ 0944222069 á‰ á‹áˆµáŒ¥ áˆˆ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text\n",
       "0  áŠ¥áŠá‹šáˆ…áŠ• áŒ«áˆ›á‹á‰½  áŠ¥áŠ“  áˆ‹á‹­ áˆ›áˆµá‰³á‹ˆá‰‚á‹« áŠ á‹­á‰³á‰½áˆ  áˆµá‰µáŒˆá‰¡ á‹«áŒ£á‰½áˆá‰µ á‰ áˆ™...\n",
       "1                                     á‹›áˆ¬ áˆ™áˆ‰á‰€áŠ• áŠ­áá‰µ áŠáŠ•\n",
       "2  áŠ¥áŠá‹šáˆ…áŠ• áŒ«áˆ›á‹á‰½  áŠ¥áŠ“  áˆ‹á‹­ áˆ›áˆµá‰³á‹ˆá‰‚á‹« áŠ á‹­á‰³á‰½áˆ  áˆµá‰µáŒˆá‰¡ á‹«áŒ£á‰½áˆá‰µ á‰ áˆ™...\n",
       "3       394041424344 á‹‹áŒ‹ 3800 áˆµáˆáŠ­ 0944222069 á‰ á‹áˆµáŒ¥ ...\n",
       "4      394041424344 á‹‹áŒ‹ 3900 áˆµáˆáŠ­ 0944222069 á‰ á‹áˆµáŒ¥ áˆˆ..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "filtered_df.to_csv(\"../data/Amharic_data.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'áŠ¥áŠá‹šáˆ…áŠ• áŒ«áˆ›á‹á‰½  áŠ¥áŠ“  áˆ‹á‹­ áˆ›áˆµá‰³á‹ˆá‰‚á‹« áŠ á‹­á‰³á‰½áˆ  áˆµá‰µáŒˆá‰¡ á‹«áŒ£á‰½áˆá‰µ á‰ áˆ™áˆ‰ á‹¨áŒ¥áˆá‰€á‰µ áŒ«áˆ›á‹á‰½ áŠá‰µ áˆˆáŠá‰µ á‰ áˆ˜áˆ†áŠ“á‰¸á‹ áˆµáˆˆáˆ†áŠ á‰µáŠ•áˆ½ áŠ¨á áŠ á‹µáˆ­áŒ á‰ áˆ˜áˆáˆˆáŒ áŠ¨ 9 áŠ¥áˆµáŠ¨ 1 á‹µáˆ¨áˆµ á–áˆµá‰µ á‹¨á‰°á‹°áˆ¨áŒ‰á‰µáŠ• áŠ¨á áŠ á‹µáˆ­áŒ á‰ áˆ˜áˆáˆˆáŒ á‹«áŒˆáŠ™á‰³áˆ'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.iloc[0][\"cleaned_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tsegaye\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\amseg-2.3-py3.13.egg\\amseg\\amharicSegmenter.py:55: SyntaxWarning: invalid escape sequence '\\s'\n",
      "c:\\Users\\Tsegaye\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\amseg-2.3-py3.13.egg\\amseg\\amharicSegmenter.py:79: SyntaxWarning: invalid escape sequence '\\s'\n",
      "c:\\Users\\Tsegaye\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\amseg-2.3-py3.13.egg\\amseg\\amharicSegmenter.py:55: SyntaxWarning: invalid escape sequence '\\s'\n",
      "c:\\Users\\Tsegaye\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\amseg-2.3-py3.13.egg\\amseg\\amharicSegmenter.py:79: SyntaxWarning: invalid escape sequence '\\s'\n",
      "c:\\Users\\Tsegaye\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\amseg-2.3-py3.13.egg\\amseg\\amharicNormalizer.py:87: SyntaxWarning: invalid escape sequence '\\s'\n",
      "c:\\Users\\Tsegaye\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\amseg-2.3-py3.13.egg\\amseg\\amharicNormalizer.py:87: SyntaxWarning: invalid escape sequence '\\s'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from amseg.amharicSegmenter import AmharicSegmenter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sent_punct = []\n",
    "word_punct = []\n",
    "segmenter = AmharicSegmenter(sent_punct, word_punct)\n",
    "\n",
    "# Define the function to tokenize Amharic text\n",
    "def tokenize_amharic(text):\n",
    "    words = segmenter.amharic_tokenizer(text)  # Tokenizing the text into words\n",
    "    return words  # Returning the tokenized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text  \\\n",
      "0  áŠ¥áŠá‹šáˆ…áŠ• áŒ«áˆ›á‹á‰½  áŠ¥áŠ“  áˆ‹á‹­ áˆ›áˆµá‰³á‹ˆá‰‚á‹« áŠ á‹­á‰³á‰½áˆ  áˆµá‰µáŒˆá‰¡ á‹«áŒ£á‰½áˆá‰µ á‰ áˆ™...   \n",
      "1                                     á‹›áˆ¬ áˆ™áˆ‰á‰€áŠ• áŠ­áá‰µ áŠáŠ•   \n",
      "2  áŠ¥áŠá‹šáˆ…áŠ• áŒ«áˆ›á‹á‰½  áŠ¥áŠ“  áˆ‹á‹­ áˆ›áˆµá‰³á‹ˆá‰‚á‹« áŠ á‹­á‰³á‰½áˆ  áˆµá‰µáŒˆá‰¡ á‹«áŒ£á‰½áˆá‰µ á‰ áˆ™...   \n",
      "3       394041424344 á‹‹áŒ‹ 3800 áˆµáˆáŠ­ 0944222069 á‰ á‹áˆµáŒ¥ ...   \n",
      "4      394041424344 á‹‹áŒ‹ 3900 áˆµáˆáŠ­ 0944222069 á‰ á‹áˆµáŒ¥ áˆˆ...   \n",
      "\n",
      "                                            tokenize  \n",
      "0  [áŠ¥áŠá‹šáˆ…áŠ•, áŒ«áˆ›á‹á‰½, áŠ¥áŠ“, áˆ‹á‹­, áˆ›áˆµá‰³á‹ˆá‰‚á‹«, áŠ á‹­á‰³á‰½áˆ, áˆµá‰µáŒˆá‰¡, á‹«áŒ£á‰½...  \n",
      "1                                [á‹›áˆ¬, áˆ™áˆ‰á‰€áŠ•, áŠ­áá‰µ, áŠáŠ•]  \n",
      "2  [áŠ¥áŠá‹šáˆ…áŠ•, áŒ«áˆ›á‹á‰½, áŠ¥áŠ“, áˆ‹á‹­, áˆ›áˆµá‰³á‹ˆá‰‚á‹«, áŠ á‹­á‰³á‰½áˆ, áˆµá‰µáŒˆá‰¡, á‹«áŒ£á‰½...  \n",
      "3  [394041424344, á‹‹áŒ‹, 3800, áˆµáˆáŠ­, 0944222069, á‰ á‹áˆµáŒ¥...  \n",
      "4  [394041424344, á‹‹áŒ‹, 3900, áˆµáˆáŠ­, 0944222069, á‰ á‹áˆµáŒ¥...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Apply the function to the 'cleaned_text' column to create a new 'tokenize' column\n",
    "filtered_df[\"tokenize\"] = filtered_df[\"cleaned_text\"].apply(tokenize_amharic)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(filtered_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'áŠ¥áŠá‹šáˆ…áŠ• áŒ«áˆ›á‹á‰½  áŠ¥áŠ“  áˆ‹á‹­ áˆ›áˆµá‰³á‹ˆá‰‚á‹« áŠ á‹­á‰³á‰½áˆ  áˆµá‰µáŒˆá‰¡ á‹«áŒ£á‰½áˆá‰µ á‰ áˆ™áˆ‰ á‹¨áŒ¥áˆá‰€á‰µ áŒ«áˆ›á‹á‰½ áŠá‰µ áˆˆáŠá‰µ á‰ áˆ˜áˆ†áŠ“á‰¸á‹ áˆµáˆˆáˆ†áŠ á‰µáŠ•áˆ½ áŠ¨á áŠ á‹µáˆ­áŒ á‰ áˆ˜áˆáˆˆáŒ áŠ¨ 9 áŠ¥áˆµáŠ¨ 1 á‹µáˆ¨áˆµ á–áˆµá‰µ á‹¨á‰°á‹°áˆ¨áŒ‰á‰µáŠ• áŠ¨á áŠ á‹µáˆ­áŒ á‰ áˆ˜áˆáˆˆáŒ á‹«áŒˆáŠ™á‰³áˆ'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.iloc[0][\"cleaned_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv(\"../data/tokenized_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
